{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Required Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk import pos_tag\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# from wordcloud import WordCloud\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01-Read Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Description</th>\n",
       "      <th>Repository URL</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>181</td>\n",
       "      <td>187</td>\n",
       "      <td>XprobePlugin</td>\n",
       "      <td>Xcode extension for viewing an application's o...</td>\n",
       "      <td>https://github.com/johnno1962/XprobePlugin</td>\n",
       "      <td>JavaScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>388</td>\n",
       "      <td>394</td>\n",
       "      <td>fluent-logger</td>\n",
       "      <td>A structured logger for Fluentd.</td>\n",
       "      <td>https://github.com/fluent/fluent-logger-d</td>\n",
       "      <td>JavaScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>484</td>\n",
       "      <td>490</td>\n",
       "      <td>oculus-d-rift</td>\n",
       "      <td>A D binding to the Oculus Rift API.</td>\n",
       "      <td>https://github.com/Circular-Studios/Oculus-D-Rift</td>\n",
       "      <td>C++</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>529</td>\n",
       "      <td>535</td>\n",
       "      <td>socket.io</td>\n",
       "      <td>Socket.Io implementation for vibe.d</td>\n",
       "      <td>https://github.com/eldar/socket.io-d</td>\n",
       "      <td>JavaScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>579</td>\n",
       "      <td>585</td>\n",
       "      <td>vibenotes</td>\n",
       "      <td>Embeddable real-time collaborative text editor</td>\n",
       "      <td>https://github.com/rejectedsoftware/vibenotes</td>\n",
       "      <td>JavaScript</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   ID           Name  \\\n",
       "0         181  187   XprobePlugin   \n",
       "1         388  394  fluent-logger   \n",
       "2         484  490  oculus-d-rift   \n",
       "3         529  535      socket.io   \n",
       "4         579  585      vibenotes   \n",
       "\n",
       "                                         Description  \\\n",
       "0  Xcode extension for viewing an application's o...   \n",
       "1                   A structured logger for Fluentd.   \n",
       "2                A D binding to the Oculus Rift API.   \n",
       "3                Socket.Io implementation for vibe.d   \n",
       "4     Embeddable real-time collaborative text editor   \n",
       "\n",
       "                                      Repository URL    Language  \n",
       "0         https://github.com/johnno1962/XprobePlugin  JavaScript  \n",
       "1          https://github.com/fluent/fluent-logger-d  JavaScript  \n",
       "2  https://github.com/Circular-Studios/Oculus-D-Rift         C++  \n",
       "3               https://github.com/eldar/socket.io-d  JavaScript  \n",
       "4      https://github.com/rejectedsoftware/vibenotes  JavaScript  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './Data/'\n",
    "\n",
    "#do this to simplify csv, make sure you have selected.csv and projects_with_repository_fields-1.6.0-2020-01-12.csv in Data folder\n",
    "#data = pd.read_csv(path + 'projects_with_repository_fields-1.6.0-2020-01-12.csv',\n",
    "                #usecols=['ID', 'Name', 'Description', 'Repository URL', 'Language'], sep=',', keep_default_na=False, index_col=False)\n",
    "\n",
    "# data.to_csv(path + 'selected.csv', index=False)\n",
    "\n",
    "\n",
    "data = pd.read_csv(path + 'selected_2.csv', sep=',', keep_default_na=False)\n",
    "# len(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02-Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://githut.info/ I picked top lang based on this, maybe we will need to clean up the data to more to include only these lang\n",
    "\n",
    "langStr = '(JavaScript)(Python)(Java)(PHP)(C++)(Ruby)'\n",
    "\n",
    "def pre_process(desc):\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    \n",
    "    # Remove non english words\n",
    "    desc = [re.sub('[^a-z' + langStr + ']', ' ', x.lower()) for x in desc]\n",
    "    # Tokenlization\n",
    "    desc_tokens = [nltk.word_tokenize(t) for t in desc]\n",
    "    # Removing Stop Words\n",
    "    desc_stop = [[t for t in tokens if (t not in stop_words) and (3 < len(t.strip()) < 15)]\n",
    "                      for tokens in desc_tokens]\n",
    "    \n",
    "    desc_stop = pd.Series(desc_stop)\n",
    "    return desc_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial preprocessing training data\n",
    "desc = data['Description']\n",
    "desc_pp = pre_process(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tokens = pd.DataFrame({'ID': list(data['ID']),\n",
    "                            'Name': list(data['Name']),\n",
    "                            'Desc_Tokens': desc_pp,\n",
    "                            'Description': list(data['Description']),\n",
    "                            'RepoUrl': list(data['Repository URL']),\n",
    "                            'Lang': list(data['Language'])\n",
    "                           })\n",
    "data_tokens.head()\n",
    "data_tokens.to_csv(path + 'tokenized_2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Desc_Tokens</th>\n",
       "      <th>Description</th>\n",
       "      <th>RepoUrl</th>\n",
       "      <th>Lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>2661</td>\n",
       "      <td>angular-django-rest-resource</td>\n",
       "      <td>[angularjs, module, provides, resource, genera...</td>\n",
       "      <td>An AngularJS module that provides a resource-g...</td>\n",
       "      <td>https://github.com/blacklocus/angular-django-r...</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5357</th>\n",
       "      <td>9284</td>\n",
       "      <td>django-autocomplete-light</td>\n",
       "      <td>[fresh, approach, autocomplete, specially, dja...</td>\n",
       "      <td>A fresh approach to autocomplete implementatio...</td>\n",
       "      <td>https://github.com/yourlabs/django-autocomplet...</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7810</th>\n",
       "      <td>12777</td>\n",
       "      <td>icinga2tst</td>\n",
       "      <td>[icinga, auto, commit, push]</td>\n",
       "      <td>for Icinga2 auto commit and push</td>\n",
       "      <td>https://github.com/alberthan/Icinga2Tst.git</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9210</th>\n",
       "      <td>14699</td>\n",
       "      <td>jquery-pjaxr</td>\n",
       "      <td>[pushstate, ajax, extended, replacements]</td>\n",
       "      <td>Pushstate aJAX eXtended Replacements</td>\n",
       "      <td>https://github.com/minddust/jquery-pjaxr.git</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10312</th>\n",
       "      <td>16109</td>\n",
       "      <td>kotti</td>\n",
       "      <td>[user, friendly, light, weight, extensible, co...</td>\n",
       "      <td>A user-friendly, light-weight and extensible w...</td>\n",
       "      <td>https://github.com/disko/Kotti.git</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                          Name  \\\n",
       "983     2661  angular-django-rest-resource   \n",
       "5357    9284     django-autocomplete-light   \n",
       "7810   12777                    icinga2tst   \n",
       "9210   14699                  jquery-pjaxr   \n",
       "10312  16109                         kotti   \n",
       "\n",
       "                                             Desc_Tokens  \\\n",
       "983    [angularjs, module, provides, resource, genera...   \n",
       "5357   [fresh, approach, autocomplete, specially, dja...   \n",
       "7810                        [icinga, auto, commit, push]   \n",
       "9210           [pushstate, ajax, extended, replacements]   \n",
       "10312  [user, friendly, light, weight, extensible, co...   \n",
       "\n",
       "                                             Description  \\\n",
       "983    An AngularJS module that provides a resource-g...   \n",
       "5357   A fresh approach to autocomplete implementatio...   \n",
       "7810                    for Icinga2 auto commit and push   \n",
       "9210                Pushstate aJAX eXtended Replacements   \n",
       "10312  A user-friendly, light-weight and extensible w...   \n",
       "\n",
       "                                                 RepoUrl    Lang  \n",
       "983    https://github.com/blacklocus/angular-django-r...  Python  \n",
       "5357   https://github.com/yourlabs/django-autocomplet...  Python  \n",
       "7810         https://github.com/alberthan/Icinga2Tst.git  Python  \n",
       "9210        https://github.com/minddust/jquery-pjaxr.git  Python  \n",
       "10312                 https://github.com/disko/Kotti.git  Python  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tokens = pd.read_csv(path + 'tokenized_2.csv', sep=',', keep_default_na=False)\n",
    "data_tokens['Desc_Tokens'] = data_tokens['Desc_Tokens'].apply(ast.literal_eval)\n",
    "data_tokens = data_tokens[data_tokens['Lang'] == 'Python']\n",
    "data_tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03-Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Desc_Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>An AngularJS module that provides a resource-g...</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5357</th>\n",
       "      <td>A fresh approach to autocomplete implementatio...</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7810</th>\n",
       "      <td>for Icinga2 auto commit and push</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9210</th>\n",
       "      <td>Pushstate aJAX eXtended Replacements</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10312</th>\n",
       "      <td>A user-friendly, light-weight and extensible w...</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Description  Desc_Length\n",
       "983    An AngularJS module that provides a resource-g...          131\n",
       "5357   A fresh approach to autocomplete implementatio...          191\n",
       "7810                    for Icinga2 auto commit and push           32\n",
       "9210                Pushstate aJAX eXtended Replacements           36\n",
       "10312  A user-friendly, light-weight and extensible w...          128"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_example = pd.DataFrame(data_tokens['Description'])\n",
    "length = data_example['Description'].apply(len)\n",
    "data_example = data_example.assign(Desc_Length=length)\n",
    "data_example.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A fresh approach to autocomplete implementations, specially for Django. Status: v3 stable, 2.x.x stable, 1.x.x deprecated. Please DO regularely ping us with your link at #yourlabs IRC channel'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Raw data\n",
    "example = data_example['Description'].iloc[1]\n",
    "raw_title = 'Raw Data'\n",
    "raw_result = example\n",
    "raw_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a fresh approach to autocomplete implementations  specially for django  status  v  stable    x x stable    x x deprecated  please do regularely ping us with your link at  yourlabs irc channel']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove non english words\n",
    "re_title = 'Remove non-English Words'\n",
    "re_result = [re.sub('[^a-z(JavaScript)(Python)(Java)(PHP)(C++)(Ruby)]', ' ', x.lower()) for x in pd.Series(example)]\n",
    "re_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a', 'fresh', 'approach', 'to', 'autocomplete', 'implementations', 'specially', 'for', 'django', 'status', 'v', 'stable', 'x', 'x', 'stable', 'x', 'x', 'deprecated', 'please', 'do', 'regularely', 'ping', 'us', 'with', 'your', 'link', 'at', 'yourlabs', 'irc', 'channel']]\n"
     ]
    }
   ],
   "source": [
    "# Tokenlization\n",
    "tk_title = 'Tokenlization'\n",
    "tk_result = [nltk.word_tokenize(t) for t in re_result]\n",
    "print(tk_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['fresh',\n",
       "  'approach',\n",
       "  'autocomplete',\n",
       "  'specially',\n",
       "  'django',\n",
       "  'status',\n",
       "  'stable',\n",
       "  'stable',\n",
       "  'deprecated',\n",
       "  'please',\n",
       "  'regularely',\n",
       "  'ping',\n",
       "  'link',\n",
       "  'yourlabs',\n",
       "  'channel']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing Stop Words\n",
    "stop_words = stopwords.words(\"english\")\n",
    "rs_title = 'Removing Stop Words'\n",
    "rs_result = [[t for t in tokens if (t not in stop_words) and (3 < len(t.strip()) < 15)] for tokens in tk_result]\n",
    "rs_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Step</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Raw Data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Remove non-English Words</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tokenlization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Removing Stop Words</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Step\n",
       "0                  Raw Data\n",
       "1  Remove non-English Words\n",
       "2             Tokenlization\n",
       "3       Removing Stop Words"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'Step' : [raw_title, re_title, tk_title, rs_title],\n",
    "        'Results' : [raw_result, re_result, tk_result, rs_result]}\n",
    "df = pd.DataFrame(data)\n",
    "cols = ['Step', 'Results']\n",
    "df = df.iloc[:,0:2]\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03-Train Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data):\n",
    "    \"\"\"Function trains and creates Word2vec Model using parsed\n",
    "    data and returns trained model\"\"\"\n",
    "    model = gensim.models.Word2Vec(train_data, min_count=5)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dict_language = {'0': 'Python', '1': 'C++', '2': 'C#', '3': 'Java', '4': 'TypeScript', '5': 'Shell', '6': 'C', \n",
    " #                '7': 'Ruby', '8': 'PHP', '9': 'JavaScript', '10': 'CSS', '11': 'Go' }\n",
    "dict_language = {'0': 'Python' }\n",
    "\n",
    "for key, value in dict_language.items():\n",
    "    desc_data = list(data_tokens[data_tokens['Lang'] == value]['Desc_Tokens'])\n",
    "\n",
    "    # Train model\n",
    "    model_name = 'word2vec_model_' + value\n",
    "    trained_model = train_model(desc_data)\n",
    "    trained_model.save(model_name)\n",
    "    print('Saved %s model successfully' % model_name)\n",
    "    \n",
    "    # Save Word2Vec model\n",
    "    word2vec_pickle_path = path + 'desc_word2vec_' + value + '.bin'\n",
    "    f = open(word2vec_pickle_path, 'wb')\n",
    "    pickle.dump(trained_model, f) \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_language = {'0': 'Python', '1': 'C++', '2': 'C#', '3': 'Java', '4': 'TypeScript', '5': 'Shell', '6': 'C', \n",
    " #                '7': 'Ruby', '8': 'PHP', '9': 'JavaScript', '10': 'CSS', '11': 'Go' }\n",
    "dict_language = {'0': 'Python' }\n",
    "\n",
    "data_tokens['Desc_Vectors'] = None\n",
    "data_tokens['Average_Pooling'] = None\n",
    "\n",
    "for key, value in dict_language.items():\n",
    "    word2vec_pickle_path = path + 'desc_word2vec_' + value + '.bin'\n",
    "    \n",
    "    model = gensim.models.KeyedVectors.load(word2vec_pickle_path)\n",
    "    \n",
    "    # Calculate the vectors for each question\n",
    "    for i in range(len(data_tokens)):\n",
    "        if data_tokens['Lang'].iloc[i] == value:\n",
    "            desc_tokens = data_tokens['Desc_Tokens'].iloc[i]\n",
    "            desc_vectors = []\n",
    "            for token in desc_tokens:\n",
    "                try:\n",
    "                    vector = model[token]\n",
    "                    desc_vectors.append(vector)\n",
    "                except:\n",
    "                    continue\n",
    "            # Vectors for each tokens\n",
    "            data_tokens['Desc_Vectors'].iloc[i] = desc_vectors\n",
    "            # Average Pooling of all tokens\n",
    "            data_tokens['Average_Pooling'].iloc[i] = list(pd.DataFrame(desc_vectors).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tokens['Desc_Tokens'] = [\" \".join(l) for l in data_tokens['Desc_Tokens']]\n",
    "length = data_tokens['Desc_Tokens'].apply(len)\n",
    "data_tokens = data_tokens.assign(Desc_Length=length)\n",
    "data_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export as data as JSON\n",
    "data_json = json.loads(data_tokens.to_json(orient='records'))\n",
    "\n",
    "with open(path + 'Desc_Word2Vec.json', 'w') as outfile:\n",
    "    json.dump(data_json, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    desc_json_path = path + 'Desc_Word2Vec.json'\n",
    "\n",
    "    with open(desc_json_path) as file:\n",
    "        reader = json.load(file)\n",
    "\n",
    "        langs = []\n",
    "        descriptions = []\n",
    "        desc_tokens = []\n",
    "        ids = []\n",
    "        names = []\n",
    "        repourls=[]\n",
    "        desc_lengths = []\n",
    "        desc_vectors = []\n",
    "        average_pooling = []\n",
    "        for row in reader:\n",
    "            lang.append(row['Lang'])\n",
    "            desc.append(row['Description'])\n",
    "            desc_tokens.append(row['Desc_Tokens'].split())\n",
    "            ids.append(row['ID'])\n",
    "            names.append(row['Name'])\n",
    "            repourls.append(row['RepoUrl'])\n",
    "            desc_lengths.append(row['Desc_Length'])\n",
    "            desc_vectors.append(row['Desc_Vectors'])\n",
    "            average_pooling.append(row['Average_Pooling'])\n",
    "\n",
    "        data_tokens = pd.DataFrame({'Lang': lang,\n",
    "                                    'Description': desc,\n",
    "                                    'Desc_Tokens': desc_tokens,\n",
    "                                    'ID': ids,\n",
    "                                    'Name': names,\n",
    "                                    'RepoUrl': repourls,\n",
    "                                    'Desc_Length': desc_lengths,\n",
    "                                    'Desc_Vectors': desc_vectors,\n",
    "                                    'Average_Pooling': average_pooling})\n",
    "except:\n",
    "    pass\n",
    "\n",
    "data_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greeting function\n",
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"hello i need help\", \"good day\",\"hey\",\"i need help\", \"greetings\")\n",
    "GREETING_RESPONSES = [\"Good day, How may i of help?\", \"Hello, How can i help?\", \"hello\", \"I am glad! You are talking to me.\"]\n",
    "           \n",
    "def greeting(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matchRepoToInput(data_by_language, model):\n",
    "    \n",
    "    # Preprocessing of user input\n",
    "    sentence_pp = pre_process(pd.Series(sentence)) \n",
    "\n",
    "    cosines = []\n",
    "    try:\n",
    "        # Get vectors and average pooling\n",
    "        question_vectors = []\n",
    "        for token in sentence_pp:\n",
    "            try:\n",
    "                vector = model[token]\n",
    "                question_vectors.append(vector)\n",
    "            except:\n",
    "                continue\n",
    "        question_ap = list(pd.DataFrame(question_vectors[0]).mean())\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        for t in data_by_language['Average_Pooling']:\n",
    "            if t is not None and len(t) == len(question_ap):\n",
    "                val = cosine_similarity([question_ap], [t])\n",
    "                cosines.append(val[0][0])\n",
    "            else:\n",
    "                cosines.append(0)\n",
    "    except:\n",
    "        pass\n",
    "            \n",
    "    # If not in the topic trained\n",
    "    if len(cosines) == 0:\n",
    "        not_understood = \"Apology, I do not understand. Can you rephrase?\"\n",
    "        return not_understood, 999\n",
    "    \n",
    "    else: \n",
    "        # Sort similarity\n",
    "        index_s =[]\n",
    "        score_s = []\n",
    "        for i in range(len(cosines)):\n",
    "            x = cosines[i]\n",
    "            if x >= 0.9:\n",
    "                index_s.append(i)\n",
    "                score_s.append(cosines[i])\n",
    "\n",
    "        reply_indexes = pd.DataFrame({'index': index_s, 'score': score_s})\n",
    "        reply_indexes = reply_indexes.sort_values(by=\"score\" , ascending=False)\n",
    "\n",
    "        # Find Top Questions and Score\n",
    "        r_index = int(reply_indexes['index'].iloc[0])\n",
    "        r_score = float(reply_indexes['score'].iloc[0])\n",
    "\n",
    "        #reply = str(data_by_language.iloc[:,0][r_index])\n",
    "        reply = str(data_by_language['Description'].iloc[r_index] + data_by_language['Name'].iloc[r_index] + data_by_language['RepoUrl'].iloc[r_index])\n",
    "        \n",
    "        return reply, r_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_language = True\n",
    "flag_query = True\n",
    "dict_language = {'0': 'Python', '1': 'C++', '2': 'C#', '3': 'Java', '4': 'TypeScript', '5': 'Shell', '6': 'C', \n",
    "                 '7': 'Ruby', '8': 'PHP', '9': 'JavaScript', '10': 'CSS', '11': 'Go', '12': 'Swift' }\n",
    "\n",
    "print('......................................................................................')\n",
    "print('\\x1b[1;37;40m' + 'Bot' + '\\x1b[0m' + ': ' + 'Hi, ask me something.')\n",
    "print('\\x1b[1;37;40m' + 'Bot' + '\\x1b[0m' + ': ' + 'If you want to exit, you can type < bye >.')\n",
    "\n",
    "while(flag_language == True):\n",
    "    print(\"......................................................................................\")\n",
    "    print('\\x1b[1;37;40m' + 'Bot' + '\\x1b[0m' + ': ' + 'Please select which language you want to enquire, ' +\n",
    "      'you can type:')\n",
    "    print('\\x1b[1;37;40m' + 'Bot' + '\\x1b[0m' + ': ' + '< 0 > for python     < 1 > for c++      < 2 > for c#')\n",
    "    print('\\x1b[1;37;40m' + 'Bot' + '\\x1b[0m' + ': ' + '< 3 > for java       < 4 > for typescript      < 5 > for shell')\n",
    "    print('\\x1b[1;37;40m' + 'Bot' + '\\x1b[0m' + ': ' + '< 6 > for c       < 7 > for ruby   < 8 > for php')\n",
    "    print('\\x1b[1;37;40m' + 'Bot' + '\\x1b[0m' + ': ' + '< 9 > for javascript       < 10 > for css   < 11 > for go')\n",
    "    print('\\x1b[1;37;40m' + 'Bot' + '\\x1b[0m' + ': ' + '< 12 > for swift')\n",
    "    print(\"......................................................................................\")\n",
    "    sentence = input('\\x1b[0;30;47m' + 'USER  ' + '\\x1b[0m' + ':')\n",
    "    print(\"......................................................................................\")\n",
    "    \n",
    "    if(sentence.lower() != 'bye'):\n",
    "        if (sentence.lower() in list(dict_language.keys())):\n",
    "            language = dict_language[sentence.lower()]\n",
    "            data_by_language = data_tokens[data_tokens['Lang'] == language]\n",
    "            data_by_language = pd.DataFrame({'Description': list(data_by_language['Description']),\n",
    "                                          'Desc_Tokens': list(data_by_language['Desc_Tokens']),\n",
    "                                          'ID': list(data_by_language['ID']),\n",
    "                                          'Name': list(data_by_language['Name']),\n",
    "                                          'RepoUrl': list(data_by_language['RepoUrl']),\n",
    "                                          'Lang': list(data_by_language['Lang']),\n",
    "                                          'Desc_Vectors': list(data_by_language['Desc_Vectors']),\n",
    "                                          'Average_Pooling': list(data_by_language['Average_Pooling'])\n",
    "                                         })\n",
    "            \n",
    "            # Read word2vec model\n",
    "            word2vec_pickle_path = path + 'desc_word2vec_' + language + '.bin'\n",
    "            model = gensim.models.KeyedVectors.load(word2vec_pickle_path)\n",
    "            \n",
    "            flag_language = False\n",
    "            flag_query = True\n",
    "    else:\n",
    "        flag_language = False\n",
    "        flag_query = False\n",
    "\n",
    "print(\"......................................................................................\")\n",
    "print('\\x1b[1;37;40m' + 'Bot' + '\\x1b[0m' + ': ' + 'Let''s start! Please input your question now.')\n",
    "    \n",
    "while(flag_query == True):\n",
    "    print(\"......................................................................................\")\n",
    "    sentence = input('\\x1b[0;30;47m' + 'Me  ' + '\\x1b[0m' + ':')\n",
    "    print(\"......................................................................................\")\n",
    "\n",
    "    if(sentence.lower() != 'bye'):\n",
    "        if(greeting(sentence.lower()) != None):\n",
    "            print('\\x1b[1;37;40m' + 'Bot' + '\\x1b[0m' + ': ' + greeting(sentence.lower()))\n",
    "        else:\n",
    "            reply, score = matchRepoToInput(data_by_language, model)\n",
    "            print('\\x1b[1;37;40m' + 'Bot'+'\\x1b[0m'+': '+reply)\n",
    "\n",
    "            #For Tracing, comment to remove from print \n",
    "            #print(\"\")\n",
    "            #print(\"SCORE: \" + str(score))\n",
    "    else:\n",
    "        flag_query = False\n",
    "print('\\x1b[1;37;40m' + 'Bot' + '\\x1b[0m' + ': ' + 'Bye! Hope that i am of help.') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
