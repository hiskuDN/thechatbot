{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Required Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk import pos_tag\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# from wordcloud import WordCloud\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01-Read Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './Data/'\n",
    "\n",
    "#do this to simplify csv, make sure you have selected.csv and projects_with_repository_fields-1.6.0-2020-01-12.csv in Data folder\n",
    "#data = pd.read_csv(path + 'projects_with_repository_fields-1.6.0-2020-01-12.csv',\n",
    "                #usecols=['ID', 'Name', 'Description', 'Repository URL', 'Language'], sep=',', keep_default_na=False, index_col=False)\n",
    "#data = data[data['Language'].isin(['JavaScript','Python', 'Java', 'PHP', 'C++'])]\n",
    "#data.to_csv(path + 'selected.csv', index=False)\n",
    "\n",
    "\n",
    "#data = pd.read_csv(path + 'selected.csv', sep=',', keep_default_na=False)\n",
    "data = pd.read_csv(path + 'libdata0.csv', sep=',', keep_default_na=False)\n",
    "len(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02-Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://githut.info/ I picked top lang based on this, maybe we will need to clean up the data to more to include only these lang\n",
    "\n",
    "langStr = '(JavaScript)(Python)(Java)(PHP)(C++)'\n",
    "\n",
    "def pre_process(desc):\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    \n",
    "    # Remove non english words\n",
    "    desc = [re.sub('[^a-z' + langStr + ']', ' ', x.lower()) for x in desc]\n",
    "    # Tokenlization\n",
    "    desc_tokens = [nltk.word_tokenize(t) for t in desc]\n",
    "    # Removing Stop Words\n",
    "    desc_stop = [[t for t in tokens if (t not in stop_words) and (3 < len(t.strip()) < 15)]\n",
    "                      for tokens in desc_tokens]\n",
    "    \n",
    "    desc_stop = pd.Series(desc_stop)\n",
    "    return desc_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initial preprocessing training data\n",
    "desc = data['description']\n",
    "desc_pp = pre_process(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tokens = pd.DataFrame({'ID': list(data['id']),\n",
    "                            'Name': list(data['name']),\n",
    "                            'Desc_Tokens': desc_pp,\n",
    "                            'Description': list(data['description']),\n",
    "                            'RepoUrl': list(data['repository_url']),\n",
    "                            'Lang': list(data['language'])\n",
    "                           })\n",
    "data_tokens.head()\n",
    "data_tokens.to_csv(path + 'tokenized.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_tokens = pd.read_csv(path + 'tokenized.csv', sep=',', keep_default_na=False)\n",
    "data_tokens['Desc_Tokens'] = data_tokens['Desc_Tokens'].apply(ast.literal_eval)\n",
    "data_tokens.head()\n",
    "#print(len(data_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03-Train Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data):\n",
    "    \"\"\"Function trains and creates Word2vec Model using parsed\n",
    "    data and returns trained model\"\"\"\n",
    "    model = gensim.models.Word2Vec(train_data, min_count=3, size=80, window=2, sg=1, hs=1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dict_language = {'0': 'Python', '1': 'C++', '2': 'C#', '3': 'Java', '4': 'TypeScript', '5': 'Shell', '6': 'C', \n",
    " #                '7': 'Ruby', '8': 'PHP', '9': 'JavaScript', '10': 'CSS', '11': 'Go' }\n",
    "dict_language = {'0': 'Python', '1': 'JavaScript', '2': 'Java', '3': 'PHP', '4': 'C++'}\n",
    "\n",
    "for key, value in dict_language.items():\n",
    "    desc_data = list(data_tokens[data_tokens['Lang'] == value]['Desc_Tokens'])\n",
    "\n",
    "    # Train model\n",
    "    model_name = 'word2vec_model_' + value\n",
    "    trained_model = train_model(desc_data)\n",
    "    trained_model.save(model_name)\n",
    "    print('Saved %s model successfully' % model_name)\n",
    "    \n",
    "    # Save Word2Vec model\n",
    "    word2vec_pickle_path = path + 'desc_word2vec_' + value + '.bin'\n",
    "    f = open(word2vec_pickle_path, 'wb')\n",
    "    pickle.dump(trained_model, f) \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_language = {'0': 'Python', '1': 'C++', '2': 'C#', '3': 'Java', '4': 'TypeScript', '5': 'Shell', '6': 'C', \n",
    " #                '7': 'Ruby', '8': 'PHP', '9': 'JavaScript', '10': 'CSS', '11': 'Go' }\n",
    "dict_language = {'0': 'Python', '1': 'JavaScript', '2': 'Java', '3': 'PHP', '4': 'C++'}\n",
    "\n",
    "#data_tokens['Desc_Vectors'] = None\n",
    "data_tokens['Average_Pooling'] = None\n",
    "\n",
    "for key, value in dict_language.items():\n",
    "    word2vec_pickle_path = path + 'desc_word2vec_' + value + '.bin'\n",
    "    \n",
    "    model = gensim.models.KeyedVectors.load(word2vec_pickle_path)\n",
    "    \n",
    "    # Calculate the vectors for each question\n",
    "    for i in range(len(data_tokens)):\n",
    "        if data_tokens['Lang'].iloc[i] == value:\n",
    "            desc_tokens = data_tokens['Desc_Tokens'].iloc[i]\n",
    "            desc_vectors = []\n",
    "            for token in desc_tokens:\n",
    "                try:\n",
    "                    vector = model[token]\n",
    "                    desc_vectors.append(vector)\n",
    "                except:\n",
    "                    continue\n",
    "            # Vectors for each tokens\n",
    "            #data_tokens['Desc_Vectors'].iloc[i] = desc_vectors\n",
    "            # Average Pooling of all tokens\n",
    "            data_tokens['Average_Pooling'].iloc[i] = list(pd.DataFrame(desc_vectors).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tokens['Desc_Tokens'] = [\" \".join(l) for l in data_tokens['Desc_Tokens']]\n",
    "length = data_tokens['Desc_Tokens'].apply(len)\n",
    "data_tokens = data_tokens.assign(Desc_Length=length)\n",
    "data_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_tokens.to_csv(path + 'tokenized_withvectors.csv', index=False)\n",
    "# Export as data as JSON\n",
    "data_json = json.loads(data_tokens.to_json(orient='records'))\n",
    "\n",
    "with open(path + 'Desc_Word2Vec.json', 'w') as outfile:\n",
    "    json.dump(data_json, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    desc_json_path = path + 'Desc_Word2Vec.json'\n",
    "    #desc_json_path = path + 'lib4_Desc_Word2Vec.json'\n",
    "\n",
    "    with open(desc_json_path) as file:\n",
    "        reader = json.load(file)\n",
    "\n",
    "        langs = []\n",
    "        descriptions = []\n",
    "        desc_tokens = []\n",
    "        ids = []\n",
    "        names = []\n",
    "        repourls=[]\n",
    "        desc_lengths = []\n",
    "        #desc_vectors = []\n",
    "        average_pooling = []\n",
    "        for row in reader:\n",
    "            langs.append(row['Lang'])\n",
    "            descriptions.append(row['Description'])\n",
    "            desc_tokens.append(row['Desc_Tokens'].split())\n",
    "            ids.append(row['ID'])\n",
    "            names.append(row['Name'])\n",
    "            repourls.append(row['RepoUrl'])\n",
    "            desc_lengths.append(row['Desc_Length'])\n",
    "            #desc_vectors.append(row['Desc_Vectors'])\n",
    "            average_pooling.append(row['Average_Pooling'])\n",
    "\n",
    "        data_tokens = pd.DataFrame({'Lang': langs,\n",
    "                                    'Description': descriptions,\n",
    "                                    'Desc_Tokens': desc_tokens,\n",
    "                                    'ID': ids,\n",
    "                                    'Name': names,\n",
    "                                    'RepoUrl': repourls,\n",
    "                                    'Desc_Length': desc_lengths,\n",
    "                                    #'Desc_Vectors': desc_vectors,\n",
    "                                    'Average_Pooling': average_pooling})\n",
    "except:\n",
    "    pass\n",
    "\n",
    "data_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greeting function\n",
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"hello i need help\", \"good day\",\"hey\",\"i need help\", \"greetings\")\n",
    "GREETING_RESPONSES = [\"Good day, How may i of help?\", \"Hello, How can i help?\", \"hello\", \"I am glad! You are talking to me.\"]\n",
    "           \n",
    "def greeting(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matchRepoToInput(data_by_language, model):\n",
    "    \n",
    "    # Preprocessing of user input\n",
    "    sentence_pp = pre_process(pd.Series(sentence)) \n",
    "\n",
    "    cosines = []\n",
    "    try:\n",
    "        # Get vectors and average pooling\n",
    "        question_vectors = []\n",
    "        for token in sentence_pp:\n",
    "            try:\n",
    "                vector = model[token]\n",
    "                question_vectors.append(vector)\n",
    "            except:\n",
    "                continue\n",
    "        question_ap = list(pd.DataFrame(question_vectors[0]).mean())\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        for t in data_by_language['Average_Pooling']:\n",
    "            if t is not None and len(t) == len(question_ap):\n",
    "                val = cosine_similarity([question_ap], [t])\n",
    "                cosines.append(val[0][0])\n",
    "            else:\n",
    "                cosines.append(0)\n",
    "    except:\n",
    "        pass\n",
    "            \n",
    "    # If not in the topic trained\n",
    "    if len(cosines) == 0:\n",
    "        not_understood = \"Apology, I do not understand. Can you rephrase?\"\n",
    "        return not_understood, 999\n",
    "    \n",
    "    else: \n",
    "        # Sort similarity\n",
    "        index_s =[]\n",
    "        score_s = []\n",
    "        for i in range(len(cosines)):\n",
    "            x = cosines[i]\n",
    "            if x >= 0.9:\n",
    "                index_s.append(i)\n",
    "                score_s.append(cosines[i])\n",
    "        \n",
    "        reply_indexes = pd.DataFrame({'index': index_s, 'score': score_s})\n",
    "        reply_indexes = reply_indexes.sort_values(by=\"score\" , ascending=False)\n",
    "        \n",
    "        if len(reply_indexes) == 0:\n",
    "            not_understood = \"Apology, I do not understand. Can you rephrase?\"\n",
    "            return not_understood, 999\n",
    "        else:\n",
    "            # Find Top Questions and Score\n",
    "            r_index = int(reply_indexes['index'].iloc[0])\n",
    "            r_score = float(reply_indexes['score'].iloc[0])\n",
    "\n",
    "            #reply = str(data_by_language.iloc[:,0][r_index])\n",
    "            reply = str(\"My suggestion: \" + data_by_language['Description'].iloc[r_index] + \". Repo name: \" + data_by_language['Name'].iloc[r_index] + \". Repo URL: \" + data_by_language['RepoUrl'].iloc[r_index])\n",
    "\n",
    "            return reply, r_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "flag_language = True\n",
    "flag_query = True\n",
    "dict_language = {'0': 'Python', '1': 'JavaScript', '2': 'Java', '3': 'PHP', '4': 'C++'}\n",
    "\n",
    "print('......................................................................................')\n",
    "print('\\x1b[1;37;40m' + 'Bot' + '\\x1b[0m' + ': ' + 'Hi, ask me something.')\n",
    "print('\\x1b[1;37;40m' + 'Bot' + '\\x1b[0m' + ': ' + 'If you want to exit, you can type < bye >.')\n",
    "\n",
    "while(flag_language == True):\n",
    "    print(\"......................................................................................\")\n",
    "    print('\\x1b[1;37;40m' + 'Bot' + '\\x1b[0m' + ': ' + 'Please select which language you want to enquire, ' +\n",
    "      'you can type:')\n",
    "    print('\\x1b[1;37;40m' + 'Bot' + '\\x1b[0m' + ': ' + '< 0 > for python     < 1 > for js      < 2 > for java')\n",
    "    print('\\x1b[1;37;40m' + 'Bot' + '\\x1b[0m' + ': ' + '< 3 > for php       < 4 > for c++')\n",
    "    print(\"......................................................................................\")\n",
    "    sentence = input('\\x1b[0;30;47m' + 'USER  ' + '\\x1b[0m' + ':')\n",
    "    print(\"......................................................................................\")\n",
    "    \n",
    "    if(sentence.lower() != 'bye'):\n",
    "        if (sentence.lower() in list(dict_language.keys())):\n",
    "            language = dict_language[sentence.lower()]\n",
    "            data_by_language = data_tokens[data_tokens['Lang'] == language]\n",
    "            data_by_language = pd.DataFrame({'Description': list(data_by_language['Description']),\n",
    "                                          'Desc_Tokens': list(data_by_language['Desc_Tokens']),\n",
    "                                          'ID': list(data_by_language['ID']),\n",
    "                                          'Name': list(data_by_language['Name']),\n",
    "                                          'RepoUrl': list(data_by_language['RepoUrl']),\n",
    "                                          'Lang': list(data_by_language['Lang']),\n",
    "                                          #'Desc_Vectors': list(data_by_language['Desc_Vectors']),\n",
    "                                          'Average_Pooling': list(data_by_language['Average_Pooling'])\n",
    "                                         })\n",
    "            \n",
    "            # Read word2vec model\n",
    "            word2vec_pickle_path = path + 'desc_word2vec_' + language + '.bin'\n",
    "            model = gensim.models.KeyedVectors.load(word2vec_pickle_path)\n",
    "            \n",
    "            flag_language = False\n",
    "            flag_query = True\n",
    "    else:\n",
    "        flag_language = False\n",
    "        flag_query = False\n",
    "\n",
    "print(\"......................................................................................\")\n",
    "print('\\x1b[1;37;40m' + 'Bot' + '\\x1b[0m' + ': ' + 'Let''s start! Please input your question now.')\n",
    "    \n",
    "while(flag_query == True):\n",
    "    print(\"......................................................................................\")\n",
    "    sentence = input('\\x1b[0;30;47m' + 'Me  ' + '\\x1b[0m' + ':')\n",
    "    print(\"......................................................................................\")\n",
    "\n",
    "    if(sentence.lower() != 'bye'):\n",
    "        if(greeting(sentence.lower()) != None):\n",
    "            print('\\x1b[1;37;40m' + 'Bot' + '\\x1b[0m' + ': ' + greeting(sentence.lower()))\n",
    "        else:\n",
    "            reply, score = matchRepoToInput(data_by_language, model)\n",
    "            print('\\x1b[1;37;40m' + 'Bot'+'\\x1b[0m'+': '+reply)\n",
    "\n",
    "            #For Tracing, comment to remove from print \n",
    "            #print(\"\")\n",
    "            #print(\"SCORE: \" + str(score))\n",
    "    else:\n",
    "        flag_query = False\n",
    "print('\\x1b[1;37;40m' + 'Bot' + '\\x1b[0m' + ': ' + 'Bye! Hope that i am of help.') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
