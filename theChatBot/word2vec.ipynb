{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Required Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk import pos_tag\n",
    "# import gensim\n",
    "# from gensim import corpora, models, similarities\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# from wordcloud import WordCloud\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01-Read Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Description</th>\n",
       "      <th>Repository URL</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>21st digital Templates</td>\n",
       "      <td>A starting point for stripped down, structured...</td>\n",
       "      <td>https://github.com/21stdigital/Xcode-Templates</td>\n",
       "      <td>Objective-C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ACCodeSnippetRepository</td>\n",
       "      <td>Synchronize code snippets with a git repository.</td>\n",
       "      <td>https://github.com/acoomans/ACCodeSnippetRepos...</td>\n",
       "      <td>Objective-C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>AdjustFontSize</td>\n",
       "      <td>Adjust font size with ⌃ + / ⌃ -</td>\n",
       "      <td>https://github.com/zats/AdjustFontSize-Xcode-P...</td>\n",
       "      <td>Objective-C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>AeroGear Template</td>\n",
       "      <td>Setup for your AeroGear projects, based on Coc...</td>\n",
       "      <td>https://github.com/aerogear/aerogear-ios-xcode...</td>\n",
       "      <td>Objective-C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>AMMethod2Implement</td>\n",
       "      <td>A simple Xcode plugin to generate implement co...</td>\n",
       "      <td>https://github.com/MellongLau/AMMethod2Implement</td>\n",
       "      <td>Objective-C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                     Name  \\\n",
       "0   1   21st digital Templates   \n",
       "1   2  ACCodeSnippetRepository   \n",
       "2   3           AdjustFontSize   \n",
       "3   4        AeroGear Template   \n",
       "4   6       AMMethod2Implement   \n",
       "\n",
       "                                         Description  \\\n",
       "0  A starting point for stripped down, structured...   \n",
       "1   Synchronize code snippets with a git repository.   \n",
       "2                    Adjust font size with ⌃ + / ⌃ -   \n",
       "3  Setup for your AeroGear projects, based on Coc...   \n",
       "4  A simple Xcode plugin to generate implement co...   \n",
       "\n",
       "                                      Repository URL     Language  \n",
       "0     https://github.com/21stdigital/Xcode-Templates  Objective-C  \n",
       "1  https://github.com/acoomans/ACCodeSnippetRepos...  Objective-C  \n",
       "2  https://github.com/zats/AdjustFontSize-Xcode-P...  Objective-C  \n",
       "3  https://github.com/aerogear/aerogear-ios-xcode...  Objective-C  \n",
       "4   https://github.com/MellongLau/AMMethod2Implement  Objective-C  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './Data/'\n",
    "\n",
    "#do this to simplify csv, make sure you have selected.csv and projects_with_repository_fields-1.6.0-2020-01-12.csv in Data folder\n",
    "#data = pd.read_csv(path + 'projects_with_repository_fields-1.6.0-2020-01-12.csv',\n",
    "                #usecols=['ID', 'Name', 'Description', 'Repository URL', 'Language'], sep=',', keep_default_na=False, index_col=False)\n",
    "\n",
    "# data.to_csv(path + 'selected.csv', index=False)\n",
    "\n",
    "\n",
    "data = pd.read_csv(path + 'selected.csv', sep=',', keep_default_na=False)\n",
    "# len(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02-Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://githut.info/ I picked top lang based on this, maybe we will need to clean up the data to more to include only these lang\n",
    "\n",
    "langStr = '(JavaScript)(Python)(Java)(PHP)(C#)(C++)(TypeScript)(Shell)(C)(Ruby)(CSS)(Go)(Swift)'\n",
    "\n",
    "def pre_process(desc):\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    \n",
    "    # Remove non english words\n",
    "    desc = [re.sub('[^a-z' + langStr + ']', ' ', x.lower()) for x in desc]\n",
    "    # Tokenlization\n",
    "    desc_tokens = [nltk.word_tokenize(t) for t in desc]\n",
    "    # Removing Stop Words\n",
    "    desc_stop = [[t for t in tokens if (t not in stop_words) and (3 < len(t.strip()) < 15)]\n",
    "                      for tokens in desc_tokens]\n",
    "    \n",
    "    desc_stop = pd.Series(desc_stop)\n",
    "    return desc_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial preprocessing training data\n",
    "desc = data['Description']\n",
    "desc_pp = pre_process(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tokens = pd.DataFrame({'ID': list(data['ID']),\n",
    "                            'Name': list(data['Name']),\n",
    "                            'Desc_Tokens': desc_pp,\n",
    "                            'Description': list(data['Description']),\n",
    "                            'RepoUrl': list(data['Repository URL']),\n",
    "                            'Lang': list(data['Language'])\n",
    "                           })\n",
    "data_tokens.head()\n",
    "data_tokens.to_csv(path + 'tokenized.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03-Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_example = pd.DataFrame(data_tokens['Description'])\n",
    "length = data_example['Description'].apply(len)\n",
    "data_example = data_example.assign(Question_Length=length)\n",
    "data_example.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data\n",
    "example = data_example['Description'][1]\n",
    "raw_title = 'Raw Data'\n",
    "raw_result = example\n",
    "raw_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non english words\n",
    "re_title = 'Remove non-English Words'\n",
    "re_result = [re.sub('[^a-z(c++)(c#)]', ' ', x.lower()) for x in pd.Series(example)]\n",
    "re_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenlization\n",
    "tk_title = 'Tokenlization'\n",
    "tk_result = [nltk.word_tokenize(t) for t in re_result]\n",
    "print(tk_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Stop Words\n",
    "stop_words = stopwords.words(\"english\")\n",
    "rs_title = 'Removing Stop Words'\n",
    "rs_result = [[t for t in tokens if (t not in stop_words) and (3 < len(t.strip()) < 15)] for tokens in tk_result]\n",
    "rs_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Step' : [raw_title, re_title, tk_title, rs_title],\n",
    "        'Results' : [raw_result, re_result, tk_result, rs_result]}\n",
    "df = pd.DataFrame(data)\n",
    "cols = ['Step', 'Results']\n",
    "df = df.ix[:,cols]\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03-Train Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data):\n",
    "    \"\"\"Function trains and creates Word2vec Model using parsed\n",
    "    data and returns trained model\"\"\"\n",
    "    model = gensim.models.Word2Vec(train_data, min_count=2)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_language = {'0': 'python', '1': 'c++', '2': 'c#', '3': 'java', '4': 'ios', '5': 'android', '6': 'html', \n",
    "                 '7': 'jquery', '8': 'php', '9': 'javascript'}\n",
    "\n",
    "data_tokens['Question_Vectors'] = None\n",
    "data_tokens['Average_Pooling'] = None\n",
    "    \n",
    "for key, value in dict_language.items():\n",
    "    questions_data = list(data_tokens[data_tokens['Class'] == value]['Question_Tokens'])\n",
    "    # Train model\n",
    "    model_name = 'word2vec_model_' + value\n",
    "    trained_model = train_model(questions_data)\n",
    "    trained_model.save(model_name)\n",
    "    print('Saved %s model successfully' % model_name)\n",
    "    \n",
    "    # Save Word2Vec model\n",
    "    word2vec_pickle_path = path + 'stackoverflow_word2vec_' + value + '.bin'\n",
    "    f = open(word2vec_pickle_path, 'wb')\n",
    "    pickle.dump(trained_model, f) \n",
    "    f.close()\n",
    "    \n",
    "    model = gensim.models.KeyedVectors.load(word2vec_pickle_path)\n",
    "    \n",
    "    # Calculate the vectors for each question\n",
    "    for i in range(len(data_tokens)):\n",
    "        if data_tokens['Class'][i] == value:\n",
    "            question_tokens = data_tokens['Question_Tokens'][i]\n",
    "            question_vectors = []\n",
    "            for token in question_tokens:\n",
    "                try:\n",
    "                    vector = model[token]\n",
    "                    question_vectors.append(vector)\n",
    "                except:\n",
    "                    continue\n",
    "            # Vectors for each tokens\n",
    "            data_tokens['Question_Vectors'][i] = question_vectors\n",
    "            # Average Pooling of all tokens\n",
    "            data_tokens['Average_Pooling'][i] = list(pd.DataFrame(question_vectors).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tokens['Question_Tokens'] = [\" \".join(l) for l in data_tokens['Question_Tokens']]\n",
    "length = data_tokens['Question_Tokens'].apply(len)\n",
    "data_tokens = data_tokens.assign(Question_Length=length)\n",
    "data_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export as data as JSON\n",
    "data_json = json.loads(data_tokens.to_json(orient='records'))\n",
    "\n",
    "with open(path + 'StackOverflow_Word2Vec.json', 'w') as outfile:\n",
    "    json.dump(data_json, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    stackoverflow_path = path + 'StackOverflow_Word2Vec.json'\n",
    "\n",
    "    with open(stackoverflow_path) as file:\n",
    "        reader = json.load(file)\n",
    "\n",
    "        classes = []\n",
    "        questions = []\n",
    "        questions_tokens = []\n",
    "        answers = []\n",
    "        question_lengths = []\n",
    "        question_vectors = []\n",
    "        average_pooling = []\n",
    "        for row in reader:\n",
    "            classes.append(row['Class'])\n",
    "            questions.append(row['Question'])\n",
    "            questions_tokens.append(row['Question_Tokens'].split())\n",
    "            answers.append(row['Answer'])\n",
    "            question_lengths.append(row['Question_Length'])\n",
    "            question_vectors.append(row['Question_Vectors'])\n",
    "            average_pooling.append(row['Average_Pooling'])\n",
    "\n",
    "        data_tokens = pd.DataFrame({'Class': classes,\n",
    "                                    'Question': questions,\n",
    "                                    'Question_Tokens': questions_tokens,\n",
    "                                    'Answer': answers,\n",
    "                                    'Question_Length': question_lengths,\n",
    "                                    'Question_Vectors': question_vectors,\n",
    "                                    'Average_Pooling': average_pooling})\n",
    "except:\n",
    "    pass\n",
    "\n",
    "data_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greeting function\n",
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"hello i need help\", \"good day\",\"hey\",\"i need help\", \"greetings\")\n",
    "GREETING_RESPONSES = [\"Good day, How may i of help?\", \"Hello, How can i help?\", \"hello\", \"I am glad! You are talking to me.\"]\n",
    "           \n",
    "def greeting(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Talk_To_Javris(data_language, model):\n",
    "    \n",
    "    # Preprocessing of user input\n",
    "    sentence_pp = pre_process(pd.Series(sentence)) \n",
    "\n",
    "    cosines = []\n",
    "    try:\n",
    "        # Get vectors and average pooling\n",
    "        question_vectors = []\n",
    "        for token in sentence_pp:\n",
    "            try:\n",
    "                vector = model[token]\n",
    "                question_vectors.append(vector)\n",
    "            except:\n",
    "                continue\n",
    "        question_ap = list(pd.DataFrame(question_vectors[0]).mean())\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        for t in data_language['Average_Pooling']:\n",
    "            if t is not None and len(t) == len(question_ap):\n",
    "                val = cosine_similarity([question_ap], [t])\n",
    "                cosines.append(val[0][0])\n",
    "            else:\n",
    "                cosines.append(0)\n",
    "    except:\n",
    "        pass\n",
    "            \n",
    "    # If not in the topic trained\n",
    "    if len(cosines) == 0:\n",
    "        not_understood = \"Apology, I do not understand. Can you rephrase?\"\n",
    "        return not_understood, 999\n",
    "    \n",
    "    else: \n",
    "        # Sort similarity\n",
    "        index_s =[]\n",
    "        score_s = []\n",
    "        for i in range(len(cosines)):\n",
    "            x = cosines[i]\n",
    "            if x >= 0.9:\n",
    "                index_s.append(i)\n",
    "                score_s.append(cosines[i])\n",
    "\n",
    "        reply_indexes = pd.DataFrame({'index': index_s, 'score': score_s})\n",
    "        reply_indexes = reply_indexes.sort_values(by=\"score\" , ascending=False)\n",
    "\n",
    "        # Find Top Questions and Score\n",
    "        r_index = int(reply_indexes['index'].iloc[0])\n",
    "        r_score = float(reply_indexes['score'].iloc[0])\n",
    "\n",
    "        reply = str(data_language.iloc[:,0][r_index])\n",
    "        \n",
    "        return reply, r_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_language = True\n",
    "flag_query = True\n",
    "dict_language = {'0': 'python', '1': 'c++', '2': 'c#', '3': 'java', '4': 'ios', '5': 'android', '6': 'html', \n",
    "                 '7': 'jquery', '8': 'php', '9': 'javascript'}\n",
    "\n",
    "print('......................................................................................')\n",
    "print('\\x1b[1;37;40m' + 'Jarvis' + '\\x1b[0m' + ': ' + 'My name is Jarvis, a Programming Language Apprentice Bot.')\n",
    "print('\\x1b[1;37;40m' + 'Jarvis' + '\\x1b[0m' + ': ' + 'I will try my best to answer your query.')\n",
    "print('\\x1b[1;37;40m' + 'Jarvis' + '\\x1b[0m' + ': ' + 'If you want to exit, you can type < bye >.')\n",
    "\n",
    "while(flag_language == True):\n",
    "    print(\"......................................................................................\")\n",
    "    print('\\x1b[1;37;40m' + 'Jarvis' + '\\x1b[0m' + ': ' + 'Please select which language you want to enquire, ' +\n",
    "      'you can type:')\n",
    "    print('\\x1b[1;37;40m' + 'Jarvis' + '\\x1b[0m' + ': ' + '< 0 > for python     < 1 > for c++      < 2 > for c#')\n",
    "    print('\\x1b[1;37;40m' + 'Jarvis' + '\\x1b[0m' + ': ' + '< 3 > for java       < 4 > for ios      < 5 > for android')\n",
    "    print('\\x1b[1;37;40m' + 'Jarvis' + '\\x1b[0m' + ': ' + '< 6 > for html       < 7 > for jquery   < 8 > for php')\n",
    "    print('\\x1b[1;37;40m' + 'Jarvis' + '\\x1b[0m' + ': ' + '< 9 > for javascript')\n",
    "    print(\"......................................................................................\")\n",
    "    sentence = input('\\x1b[0;30;47m' + 'USER  ' + '\\x1b[0m' + ':')\n",
    "    print(\"......................................................................................\")\n",
    "    \n",
    "    if(sentence.lower() != 'bye'):\n",
    "        if (sentence.lower() in list(dict_language.keys())):\n",
    "            language = dict_language[sentence.lower()]\n",
    "            data_language = data_tokens[data_tokens['Class'] == language]\n",
    "            data_language = pd.DataFrame({'Question': list(data_language['Question']),\n",
    "                                          'Question_Tokens': list(data_language['Question_Tokens']),\n",
    "                                          'Answer': list(data_language['Answer']),\n",
    "                                          'Class': list(data_language['Class']),\n",
    "                                          'Question_Vectors': list(data_language['Question_Vectors']),\n",
    "                                          'Average_Pooling': list(data_language['Average_Pooling'])\n",
    "                                         })\n",
    "            \n",
    "            # Read word2vec model\n",
    "            word2vec_pickle_path = path + 'stackoverflow_word2vec_' + language + '.bin'\n",
    "            model = gensim.models.KeyedVectors.load(word2vec_pickle_path)\n",
    "            \n",
    "            flag_language = False\n",
    "            flag_query = True\n",
    "    else:\n",
    "        flag_language = False\n",
    "        flag_query = False\n",
    "\n",
    "print(\"......................................................................................\")\n",
    "print('\\x1b[1;37;40m' + 'Jarvis' + '\\x1b[0m' + ': ' + 'Let''s start! Please input your question now.')\n",
    "    \n",
    "while(flag_query == True):\n",
    "    print(\"......................................................................................\")\n",
    "    sentence = input('\\x1b[0;30;47m' + 'USER  ' + '\\x1b[0m' + ':')\n",
    "    print(\"......................................................................................\")\n",
    "\n",
    "    if(sentence.lower() != 'bye'):\n",
    "        if(greeting(sentence.lower()) != None):\n",
    "            print('\\x1b[1;37;40m' + 'JARVIS' + '\\x1b[0m' + ': ' + greeting(sentence.lower()))\n",
    "        else:\n",
    "            reply, score = Talk_To_Javris(data_language, model)\n",
    "            print('\\x1b[1;37;40m' + 'JARVIS'+'\\x1b[0m'+': '+reply)\n",
    "\n",
    "            #For Tracing, comment to remove from print \n",
    "            #print(\"\")\n",
    "            #print(\"SCORE: \" + str(score))\n",
    "    else:\n",
    "        flag_query = False\n",
    "print('\\x1b[1;37;40m' + 'JARVIS' + '\\x1b[0m' + ': ' + 'Bye! Hope that i am of help.') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
